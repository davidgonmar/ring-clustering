\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{float}


\usetikzlibrary{calc}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Clustering Rings in Noisy Data}

\author{\IEEEauthorblockN{1\textsuperscript{st} David González Martínez}

\IEEEauthorblockA{\textit{University of Seville} \\
Seville, Spain \\
davgonmar2@alum.us.es}
}
\maketitle

\begin{abstract}
In this paper, we apply the Fuzzy K-Rings algorithm, also known as the Fuzzy C-Shells algorithm to ring clustering in noisy data.
We further propose a modification to the algorithm to make it more robust to noise, and conduct different experiments to test the performance of the algorithm.
\end{abstract}

\begin{IEEEkeywords}
Fuzzy K-Rings, Fuzzy C-Shells, Clustering, Noisy Data, Ring Clustering, Hypersphere Clustering
\end{IEEEkeywords}

\section{Introduction}
We are presented with the following problem: we have a dataset that is composed different rings and noise. Our objective is to classify the points in different clusters,
each corresponding to a different ring. The Fuzzy K-Rings algorithm, also known as the Fuzzy C-Shells algorithm, is a clustering algorithm that has been used in the past
for similar tasks. The algorithm is inspired on the Fuzzy C-Means algorithm, and was introduced, altough in different variations, in \cite{308484} and \cite{DAVE1992713}.
From now on, we'll refer to the algorithm as Fuzzy K-Rings (FKR).
% here a plot from plots/noisy_rings.pgf
\begin{figure}[H]
    \centering
    \resizebox{0.9\linewidth}{!}{\input{plots/noisy_rings.pgf}}
    \label{fig:noisy_rings}
    \caption{Example of a dataset with different rings and noise, and their correct classification.}
\end{figure}


\section{Fuzzy Clustering Algorithms}
In hard clustering algorithms, each data point is assigned to a single cluster. In contrast, fuzzy algorithms assign a membership degree to each data point for each cluster.
One of the most popular ones is the Fuzzy C-Means algorithm. A formal description can be found in \cite{bookpatternrecognition} and \cite{BEZDEK1984191}. It can be seen as an optimization problem,
where the objective function is to minimize the following equation:
\begin{equation}
J(U, V) = \sum_{i=1}^{n} \sum_{j=1}^{k} (u_{ij})^q d_{ij}^2
\end{equation}
where $k$ is the number of clusters, $n$ is the number of data samples, $u_{ij}$ is the membership degree of cluster $i$ to data sample $j$,
$d_{ij}$ is the eucledian distance between data point $i$ and the center of cluster $j$, and $q$ is a parameter that controls the fuzziness of the membership degrees.
That is, higher values of $q$ will make the membership degrees more 'fuzzy', and lower values will make them harder, that is, closer to regular K-Means.
$u$ is a matrix of size $n \times k$, and can be interpreted as 'how much data point $i$ belongs to cluster $j$'.
It is important to note that the following conditions must be met, as described in \cite{BEZDEK1984191}:
\begin{enumerate}
    \item $u_{ij} \in [0, 1]$
    \item $\sum_{j=1}^{k} u_{ij} = 1$
\end{enumerate}
The higher the value of $q$, the more 'fuzzy' the algorithm will be. In the limit, when $q \rightarrow 1$, the algorithm will be equivalent to K-Means.


\section{The Fuzzy K-Rings Algorithm}
The Fuzzy K-Rings algorithm is a clustering algorithm that is able to cluster data points in a ring-shaped dataset.
The algorithm is inspired on the K-Means algorithm, and described in \cite{DAVE1992713} and \cite{308484}, altough in different variations.
It is described as an optimization problem, where the objective function is to minimize the following equation:
\begin{equation}\label{eq:objective}
J_q(U, V) = \sum_{i=1}^{n} \sum_{j=1}^{k} u_{ij}^q (d_{ij} - r_i)^2
\end{equation}
where $k$ is the number of clusters, $n$ is the number of data samples, $u_{ij}$ is the membership degree of cluster $i$ to data sample $j$, $d_{ij}$ is the eucledian distance between data point $i$ and the center of cluster $j$, $r_i$ is the radius of the cluster $i$, and $q$ is a parameter that controls the fuzziness of the membership degrees.
That is, higher values of $q$ will make the membership degrees more fuzzy, and lower values will make them harder.
From now on, we'll refer to $$(d_{ij} - r_i)^2$$ as $d'_ij$.
Now, we'll describe the ways to update the different parameters in the algorithm, and then we'll describe the initialization and convergence criteria, as well as the concrete steps.

\subsection{Updating the Membership Degrees}
The membership degrees are updated using the following equation, as described in both \cite{DAVE1992713} and \cite{308484}. It's the same as the one used in the Fuzzy C-Means algorithm, but with $d_{ij}$ replaced by $d'_{ij}$.
\begin{equation}
u_{ij} = \frac{d'^2(X_j, V_i)^{\frac{-1}{q-1}}}{\sum_{k=1}^{K} d'^2(X_j, V_k)^{\frac{-1}{q-1}}}
\end{equation}

\subsection{Updating the Cluster Radii and Centers}
As mentioned, we can define the algorith as an optimization problem after fixing $U$. We can then obtain the optimal (minimum) values for the objective function
by setting the partial derivatives with respect to $r_i$ and $V_i$ to zero.
First, we have:
\begin{equation}\label{eq:d_dr}
\frac{\partial}{\partial r_i}(J_q) = \sum_{j=1}^{n} u_{ij}^q\frac{\partial}{\partial r_i} (d_{ij} - r_i)^2 = \sum_{j=1}^{n} u_{ij}^q (r_i - d_{ij}) = 0
\end{equation}

Here, we take a different approach to the one taken in \cite{308484}, and similar to the one in \cite{DAVE1992713}. It allows vectorized computations, and
automatic extension to higher dimensions, unlike \cite{308484}.

\begin{tikzpicture}\label{fig:circle}
    % Define radius
    \def\radius{3cm}
    \def\angle{45}

    % Draw the circle
    \draw (0,0) circle (\radius);

    % Define points
    \coordinate (V_i) at (0,0); % Center of the circle
    \coordinate (A) at ({\radius*cos(\angle)},0); % Point on the circle at the specified angle
    \coordinate (V'_i) at (\angle:\radius);
    \coordinate (X_j) at (3.5, 3.5);

    % Draw the triangle
    \draw (V_i) -- (A) -- (V'_i) -- cycle;
    \draw[dotted] (V'_i) -- (X_j);

    % Draw points
    \fill (V_i) circle (2pt);
    \fill (V'_i) circle (2pt);
    \fill (X_j) circle (2pt);

    \node[below] at (V_i) {$V_i$};
    \node[above right] at (V'_i) {$V'_i$};
    \node[above right] at (X_j) {$X_j$};

    % label on hypotenuse (line that connects V_i and V'_i). Not on the point, on the line
    \node[above=2pt] at ($(V_i)!0.5!(V'_i)$) {$r_i$};
    % explanation
    \node[below] at (0,-\radius-0.5) {
        \begin{tabular}{c}
            $V'_i = \frac{r_i}{d_{ij}}X_j + (1 - \frac{r_i}{d_{ij}})V_i$ \\
        \end{tabular}
    };
\end{tikzpicture}
\begin{center}
\textbf{Figure \ref{fig:circle}:} Illustration of the update of the cluster centers equation when $X_j$ is outside the circle.
\end{center}


Let $X_j$ be a data point, and $V_i$ be the center of cluster $i$. Let $d_{ij}$ be the eucledian distance between $X_j$ and $V_i$,
and $r_i$ be the radius of cluster $i$.
Let $d'_{ij}$ be the distance between $X_j$ and the circle with center $V_i$ and radius $r_i$.

Then, let the following be true:
\begin{equation}
V'_i = \frac{r_i}{d_{ij}}X_j + (1 - \frac{r_i}{d_{ij}})V_i
\end{equation}

Differentiating \eqref{eq:objective} with respect to $V_i$ and setting it to zero, we get:
\begin{equation}\label{eq:d_dV}
\frac{\partial}{\partial V_i}(J_q) = \sum_{j=1}^{n} u_{ij}^q\frac{\partial}{\partial V_i} (d'_{ij})^2 = 0
\end{equation}
Note that we can rewrite $(d'_{ij})^2$ as $(X_j - V'_i)^T(X_j - V'_i)$.
Then, we can solve:
\begin{equation}
\begin{aligned}
\frac{\partial}{\partial V_i} (d_{ij} - r_i)^2 &= \left(\frac{\partial}{\partial V_i} (|X_j - V_i| - r_i)^2\right) \\
&= -2 \left( (X_j - V_i) - \frac{r_i}{|X_j - V_i|} (X_j - V_i) \right) \\
&= -2 \left( (X_j - V_i) - \frac{r_i}{d_{ij}} (X_j - V_i) \right) \\
&= -2 \left( (1 - \frac{r_i}{d_{ij}})X_J - (1 - \frac{r_i}{d_{ij}})V_i \right)
\end{aligned}
\end{equation}
Let's define $(1 - \frac{r_i}{d_{ij}})$ as $\alpha_{ij}$.
Then, we can rewrite the equation as:
\begin{equation}
\frac{\partial}{\partial V_i} (d_{ij} - r_i)^2 = -2 \alpha_{ij} (X_j - V_i)
\end{equation}
Plugging that into \eqref{eq:d_dV}, we get:
\begin{equation}
\sum_{j=1}^{n} u_{ij}^q (d_{ij} - r_i) (X_j - V_i) = 0
\end{equation}
We now have a system of equations that we can solve for $V_i$ and $r_i$ to obtain the critical points of the objective function. It is important that since
the equations are coupled, they must be solved together, since it yields better results. \cite{DAVE1992713} mentions (and cites the proof) that, indeed, the critical
points are minima. The experimental results also back this up.
One solution, as noted in \cite{DAVE1992713}, is:
\begin{equation}
V_i = \frac{\sum_{j=1}^{n} u_{ij}^q X_j}{\sum_{j=1}^{n} u_{ij}^q}
\end{equation}
\begin{equation}\label{eq:r_i}
r_i = \frac{\sum_{j=1}^{n} u_{ij}^q d_{ij}}{\sum_{j=1}^{n} u_{ij}^q}
\end{equation}
On the other hand, \cite{308484} proposes a different solution, which is to solve the equations separately. In my experiments, I found that solution not to work very
well in practice, and the one proposed by \cite{DAVE1992713} to work better.
It is also easily checkable that the solution proposed by \cite{DAVE1992713} is a critical point by substituting it into the equations.


\subsection{Intuition}
After having given a formal description of the algorithm, we can give an intuitive explanation of the algorithm.

First, for the membership degrees, we can see that the algorithm is trying to assign higher membership degrees to points that are closer to the ring contour.
This is done by averaging the distances of the points to the different rings, and assigning them proportionally.

As for the cluster centers, the algorithm is just using a weighted average of the points, with the weights being the membership degrees.
This is similar to the K-Means algorithm, but with the weights being the membership degrees instead of a binary value, and exactly the same as the Fuzzy C-Means algorithm.

Finally, for the radii, the algorithm is trying to assign the radii by using a weighted average of the distances of the points to the cluster centers.
This is done by using the membership degrees as weights, and the distances as the values to be averaged.


\subsection{Initializing the Parameters and Nature of the Data}
\cite{308484} proposes two initialization methods, depending on the nature of the data. We adopt both:
\begin{itemize}
    \item Concentric datasets: In this case, since all rings share the same center, but have different radii, the procedure is as follows.
    For the centers, we simply compute the baricenter of the dataset:
    \begin{equation}
        V_i = \frac{1}{n} \sum_{j=1}^{n} X_j
    \end{equation}
    Then, for the radii we define max and min as follows:
    \begin{equation}
        r_{\text{max}} = \max_{j} d(X_j, V_i)
    \end{equation}
    \begin{equation}
        r_{\text{min}} = \min_{j} d(X_j, V_i)
    \end{equation}
    They denote the maximum and minimum distance of a point to the center. Then, we can initialize the radii as sampling from a uniform distribution:
    \begin{equation}
        r_i = r_{\text{min}} + (r_{\text{max}} - r_{\text{min}}) \cdot \text{rand()}
    \end{equation}
    Which will generate a random radius between the minimum and maximum distance of a point to the center.
    \item Non-concentric datasets: In this case, the rings do not share the same center. The rings can also interlock, and their radii are usually different.
    As described in \cite{308484}, we first run the Fuzzy C-Means algorithm on the dataset. After that, we directly use the membership degrees and centers as our initial
    stato. As for the radius, we obtain it with equation \eqref{eq:r_i}.
\end{itemize}

\subsection{Convergence Criterion}
We use the following convergence criterion:
\begin{equation}
|\hat{u_{ij}} - u_{ij}| < \epsilon \quad \forall i, j
\end{equation}
Where $\hat{u_{ij}}$ is the membership degree of the previous iteration, and $u_{ij}$ is the membership degree of the current iteration,
and $\epsilon$ is a small value, usually $10^{-3}$, given as a hyperparameter.
That is, after each update, we check for the difference between the membership degrees of the current and previous iteration, and if the difference is smaller than $\epsilon$,
we break the loop. We do not stop it completely, but we will get to that later.

\subsection{Background noise detection}
In the case of noisy data, the algorithm can be sensitive to noise. To mitigate this, we propose an aditional step.
\begin{figure}[H]
    \centering
    \resizebox{0.9\linewidth}{!}{\input{plots/noisy_bg.pgf}}
    \label{fig:noisy_bg}
    \caption{Example of a dataset with different rings and background noise, and their correct classification (purple means noise).}
\end{figure}
The algorithm takes an additional noise threshold, which is the maximum distance a point can have to a cluster center to be considered noise.
We can express the equation as:
\begin{equation}
    \text{is\_noise}(X_j) = \begin{cases}
        1 & \text{if } \min_{i} d_{ij} > \text{noise\_distance\_threshold} \\
        0 & \text{otherwise}
    \end{cases}
\end{equation}
When computing the centers and radii, points that are considered noise are ignored, that is, all their weights are set to zero.
It is important to note that it is only for the computation of the centers and radii. The stored membership degrees are not modified.
This is done with the use of a mask, where 1 means not noise, and 0 means noise, and then multiplying the mask by the membership degrees and the distances.


\subsection{Overall Steps}
The overall steps of the algorithm are as follows:
\begin{enumerate}
    \item Initialize the parameters $U$, $V$, and $R$.
    \item While iter < max\_iter:
    \begin{enumerate}
        \item Update the membership degrees $U$.
        \item Update the cluster radii $R$ and centers $V$ using the equations \eqref{eq:d_dr} and \eqref{eq:r_i}.
        \item If convergence criterion is met, compute the noise mask, and continue to the next step.
        \item If convergence criterion is met, and the noise mask is not the same as the old one, go to the start of the loop.
        \item Else, break the loop.
    \end{enumerate}
    \item Return the membership degrees $U$, the cluster centers $V$, and the cluster radii $R$.
\end{enumerate}

\subsection{Obtaining the results}
After the algorithm has converged or we have reached the maximum number of iterations, we can obtain the results.
Recall that the membership degree can be seen as 'how much a point belongs to a cluster', and each vector can be seen as a probability distribution.
Having this in mind, there are multiple ways we could obtain the results:
\begin{enumerate}
    \item We can assign each point to the cluster with the highest membership degree.
    \item We can sample from a multinomial distribution with the membership degrees as the probabilities.
    \item We can simply use the membership degrees directly.
\end{enumerate}
We chose the first option, that is, assigning each point to the cluster with the highest membership degree.
As for the radius and the center, obtaining them is a direct result of the algorithm, and we can use them directly.

\subsection{Implementation}
\subsubsection{Data Structures}
We implement the algorithm in Python, using the NumPy \cite{harris2020array} library. The state of the algorithm is stored in three matrices:
\begin{itemize}
    \item $U$: A matrix of size $k \times n$, where $n$ is the number of data samples, and $k$ is the number of clusters. It stores the membership degrees.
    \item $V$: A matrix of size $k \times d$, where $d$ is the number of dimensions of the data samples. It stores the cluster centers.
    \item $R$: A vector of size $k$. It stores the cluster radii.
\end{itemize}
\subsubsection{Pseudocode}
All of the above steps can be formulated as Tensor/NDArray operations, with no need for explicit loops,
by defining the different operations as element-wise operations, tensor multiplications and exploiting
expansion and broadcasting rules as needed. Code is shown at \cite{github}.
This allows to exploit the parallelism of modern computing frameworks, like NumPy, Eigen, or PyTorch.
The algorithm can be implemented with the following pseudocode:
\begin{verbatim}
fn fkr(X, k, q, epsilon):
    U, V, R = initialize(X, k)
    while True:
        U = update_membership(X, V, q)
        R = update_radii(X, U, q)
        V = update_centers(X, U, R, q)
        if converged(U, U_old, epsilon):
            break
    return U, V, R
\end{verbatim}
\begin{verbatim}
fn update_membership(X, V, q):
    D = distance_matrix(X, V)
    U = D ** (-1 / (q - 1))
    U = U / U.sum(axis=1)
    return u
\end{verbatim}

\section{Experiments}
We conducted different experiments to test the performance of the algorithm. Given a dataset, that is, a set of points, we could define, informally,
two types of points, those that belong to a ring, and those that don't belong to any. We call the second one 'background noise' or 'noise' from now on.
In order to generate the dataset, we generate N rings with n noise. The noise of the rings can be seen as 'imperfection' that would occur in a real dataset.
Moreover, we generate $N_2$ noise points randomly accross the space.
\subsection{Evaluation Metrics}
We use the following metrics to evaluate the performance of the algorithm:
\begin{itemize}
    \item Squared distance error (with hard labels)
    \begin{equation}
        \text{SDE} = \sum_{i=1}^{n} distance(X_i, ring_i)^2
    \end{equation}
    Where $distance(X_i, ring_i)$ is the distance between the point $X_i$ and the circunference of the ring $ring_i$.
    $ring_i$ denotes the classified ring of the point $X_i$.
    Recall that we could classify some points as noise. In the case of a noise point, distance returns 0, that is,
    we do not take noise points into account when computing the SDE.
\end{itemize}
On the other hand, we are also interested in getting the lowest runtime possible. We measure the runtime of the algorithm, as well as the total number of iterations it takes to converge.

\subsection{Results}
We conducted different experiments to test the performance of the algorithm. We noticed that, given no background noise, the algorithm performs well with

\bibliographystyle{IEEEtran}
\bibliography{references.bib}

\vspace{12pt}
\color{red}
\end{document}
